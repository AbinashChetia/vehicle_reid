{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Re-identification using Transformer and Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from dataset import VehicleReIDDataset\n",
    "from vit import VisionTransformer\n",
    "from loss import TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.backends.mps.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hard_triplets(embeddings, labels):\n",
    "    '''\n",
    "    Selects hard positives and negatives based on Euclidean distances.\n",
    "    '''\n",
    "    labels = labels.cpu().numpy()\n",
    "    pairwise_distances = torch.cdist(embeddings, embeddings)  # Compute pairwise distances\n",
    "\n",
    "    hard_triplets = []\n",
    "    for i in range(len(labels)):\n",
    "        anchor_idx = i\n",
    "        anchor_label = labels[i]\n",
    "\n",
    "        # Hard positive: Closest with same label\n",
    "        positive_indices = np.where(labels == anchor_label)[0]\n",
    "        positive_indices = positive_indices[positive_indices != i]  # Exclude self\n",
    "        if len(positive_indices) == 0:\n",
    "            continue\n",
    "        positive_idx = positive_indices[torch.argmin(pairwise_distances[i, positive_indices])]\n",
    "\n",
    "        # Hard negative: Furthest with different label\n",
    "        negative_indices = np.where(labels != anchor_label)[0]\n",
    "        negative_idx = negative_indices[torch.argmax(pairwise_distances[i, negative_indices])]\n",
    "\n",
    "        hard_triplets.append((anchor_idx, positive_idx, negative_idx))\n",
    "\n",
    "    return hard_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_fn, device, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Extract embeddings\n",
    "            embeddings = model(images)\n",
    "\n",
    "            # Hard triplet selection\n",
    "            triplets = get_hard_triplets(embeddings, labels)\n",
    "            if len(triplets) == 0:\n",
    "                continue\n",
    "\n",
    "            anchor, positive, negative = zip(*triplets)\n",
    "            anchor = torch.stack([embeddings[i] for i in anchor])\n",
    "            positive = torch.stack([embeddings[i] for i in positive])\n",
    "            negative = torch.stack([embeddings[i] for i in negative])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(anchor, positive, negative)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, dataloader, device):\n",
    "    '''\n",
    "    Extracts embeddings for images in the dataset.\n",
    "    '''\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    return torch.cat(all_embeddings), np.array(all_labels)\n",
    "\n",
    "def retrieve_top_k(embedding, dataset_embeddings, dataset_labels, k=5):\n",
    "    '''\n",
    "    Retrieve top-k similar images using Euclidean distance.\n",
    "    '''\n",
    "    distances = torch.cdist(embedding.unsqueeze(0), dataset_embeddings)\n",
    "    top_k_indices = torch.argsort(distances, dim=1)[0][:k]\n",
    "    return dataset_labels[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = VehicleReIDDataset(root_dir='path/to/dataset', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load ViT model (assuming a ViT class is implemented)\n",
    "model = VisionTransformer(embed_dim=512)  # Adjust based on your implementation\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "loss_fn = TripletLoss(margin=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "train(model, dataloader, optimizer, loss_fn, DEVICE, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for inference\n",
    "dataset_embeddings, dataset_labels = extract_embeddings(model, dataloader, DEVICE)\n",
    "\n",
    "# Example inference: Get top-k matches for a new image\n",
    "query_image, _ = dataset[0]  # Example query image\n",
    "query_embedding = model(query_image.unsqueeze(0).to(DEVICE)).cpu()\n",
    "top_k_matches = retrieve_top_k(query_embedding, dataset_embeddings, dataset_labels, k=5)\n",
    "\n",
    "print('Top-5 Retrieved Labels:', top_k_matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vehicle-reid-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
