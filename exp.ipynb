{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Re-identification using Transformer and Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from dataset import VehicleReIDDataset, dataloader_train_test_split\n",
    "from vit import ViTEncoder\n",
    "from loss import TripletLoss\n",
    "from utils import get_hard_triplets, extract_embeddings\n",
    "from performance_analysis import compute_cmc_rankn, compute_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaizing model\n",
    "model = ViTEncoder(\n",
    "    img_size=224,\n",
    "    patch_size=8,\n",
    "    in_chans=3,\n",
    "    embed_dim=512,\n",
    "    depth=4,\n",
    "    n_heads=4,\n",
    "    mlp_ratio=4.,\n",
    "    qkv_bias=True,\n",
    "    p=0.1,\n",
    "    attn_p=0.1,\n",
    "    out_dim=256\n",
    ")\n",
    "\n",
    "# defining transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((384, 384), interpolation=3),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.225, 0.225, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_fn, device, val_loader=None, epochs=10):\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        #################### Train ####################\n",
    "        model.train()\n",
    "        train_epoch_loss = 0\n",
    "        print('Training...')\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Extract embeddings\n",
    "            embeddings = model(images)\n",
    "\n",
    "            # Hard triplet selection\n",
    "            triplets = get_hard_triplets(embeddings, labels)\n",
    "            if len(triplets) == 0:\n",
    "                continue\n",
    "\n",
    "            anchor, positive, negative = zip(*triplets)\n",
    "            anchor = torch.stack([embeddings[i] for i in anchor])\n",
    "            positive = torch.stack([embeddings[i] for i in positive])\n",
    "            negative = torch.stack([embeddings[i] for i in negative])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(anchor, positive, negative)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_epoch_loss += loss.item()\n",
    "\n",
    "        train_epoch_loss /= len(train_loader)\n",
    "        train_loss_hist.append(train_epoch_loss)\n",
    "\n",
    "        #################### Validation ####################\n",
    "        if val_loader:\n",
    "\n",
    "            model.eval()\n",
    "            val_epoch_loss = 0\n",
    "            print('Validating...')\n",
    "            for images, labels in tqdm(val_loader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Extract embeddings\n",
    "                embeddings = model(images)\n",
    "\n",
    "                # Hard triplet selection\n",
    "                triplets = get_hard_triplets(embeddings, labels)\n",
    "                if len(triplets) == 0:\n",
    "                    continue\n",
    "\n",
    "                anchor, positive, negative = zip(*triplets)\n",
    "                anchor = torch.stack([embeddings[i] for i in anchor])\n",
    "                positive = torch.stack([embeddings[i] for i in positive])\n",
    "                negative = torch.stack([embeddings[i] for i in negative])\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(anchor, positive, negative)\n",
    "\n",
    "                val_epoch_loss += loss.item()\n",
    "\n",
    "            val_epoch_loss /= len(val_loader)\n",
    "            val_loss_hist.append(val_epoch_loss)\n",
    "\n",
    "        #################### Log ####################\n",
    "\n",
    "        print(f'Epoch {epoch+1:3}/{epochs:3} | Train Loss: {train_epoch_loss:.4f} | Validation Loss: {val_epoch_loss:.4f}')\n",
    "    \n",
    "    return train_loss_hist, val_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "# vric_dataset = VehicleReIDDataset(root_dir='data/VRIC_dataset/train_images', labels_file='data/VRIC_dataset/vric_train.txt', name='VRIC', transform=transform)\n",
    "veri_carla_dataset = VehicleReIDDataset(root_dir='data/VeRi_CARLA_dataset/image_train', name='VeRi_CARLA', transform=transform)\n",
    "train_loader, val_loader = dataloader_train_test_split(veri_carla_dataset, batch_size=32, test_split=0.2)\n",
    "\n",
    "# model.load_state_dict(torch.load('trained_models/model_10_veri_carla.pth', map_location=DEVICE)) # to initialize model with pre-trained weights\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "loss_fn = TripletLoss(margin=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 398/1274 [02:31<05:47,  2.52it/s]"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_loss_hist, val_loss_hist = train(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    optimizer=optimizer, \n",
    "    loss_fn=loss_fn, \n",
    "    device=DEVICE,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss_hist, label='Train Loss', color='r')\n",
    "plt.plot(val_loss_hist, label='Validation Loss', color='b')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "torch.save(model.state_dict(), 'trained_models/model_10_veri_carla.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('trained_models/model_10_veri_carla.pth', map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, image_path, device=DEVICE, transform=transform):\n",
    "    '''\n",
    "    Extracts embeddings for a single image.\n",
    "    '''\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image)\n",
    "    return embedding.cpu().numpy().flatten()\n",
    "\n",
    "trial_image_path = './data/VeRi_CARLA_dataset/image_gallery/20220710050038_0_44.jpg'\n",
    "infer(model, trial_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('trained_models/model_10_veri_carla.pth', map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_carla_query = VehicleReIDDataset(root_dir='data/VeRi_CARLA_dataset/image_query', name='VeRi_CARLA', transform=transform)\n",
    "veri_carla_gallery = VehicleReIDDataset(root_dir='data/VeRi_CARLA_dataset/image_gallery', name='VeRi_CARLA', transform=transform)\n",
    "\n",
    "# vric_query = VehicleReIDDataset(root_dir='data/VRIC_dataset/probe_images', labels_file='data/VRIC_dataset/vric_probe.txt', name='VRIC', transform=transform)\n",
    "# vric_gallery = VehicleReIDDataset(root_dir='data/VRIC_dataset/gallery_images', labels_file='data/VRIC_dataset/vric_gallery.txt', name='VRIC', transform=transform)\n",
    "\n",
    "query_loader = DataLoader(veri_carla_query, batch_size=1, shuffle=False)\n",
    "gallery_loader = DataLoader(veri_carla_gallery, batch_size=1, shuffle=False)\n",
    "\n",
    "query_embeddings, query_labels = extract_embeddings(model, query_loader, DEVICE)\n",
    "gallery_embeddings, gallery_labels = extract_embeddings(model, gallery_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "similarity = cosine_similarity(query_embeddings, gallery_embeddings)\n",
    "\n",
    "map_score = compute_map(similarity, query_labels, gallery_labels)\n",
    "print(f'Mean Average Precision (mAP): {map_score * 100:.2f}%')\n",
    "\n",
    "cmc_curve, rank_1, rank_5, rank_10 = compute_cmc_rankn(similarity, query_labels, gallery_labels)\n",
    "\n",
    "print(f'Rank-1 Accuracy: {rank_1:.2f}%')\n",
    "print(f'Rank-5 Accuracy: {rank_5:.2f}%' if rank_5 else '')\n",
    "print(f'Rank-10 Accuracy: {rank_10:.2f}%' if rank_10 else '')\n",
    "\n",
    "ranks = np.arange(1, len(cmc_curve) + 1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(ranks, cmc_curve, marker='o', linestyle='-', label='CMC Curve')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Matching Accuracy')\n",
    "plt.title('CMC Curve for Vehicle Re-Identification')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vehicle-reid-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
