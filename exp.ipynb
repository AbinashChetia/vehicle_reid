{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Re-identification using Transformer and Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from dataset import VehicleReIDDataset\n",
    "from vit import ViTEncoder\n",
    "from loss import TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hard_triplets(embeddings, labels):\n",
    "    '''\n",
    "    Selects hard positives and negatives based on Euclidean distances.\n",
    "    '''\n",
    "    labels = labels.cpu().numpy()\n",
    "    pairwise_distances = torch.cdist(embeddings, embeddings)  # Compute pairwise distances\n",
    "\n",
    "    hard_triplets = []\n",
    "    for i in range(len(labels)):\n",
    "        anchor_idx = i\n",
    "        anchor_label = labels[i]\n",
    "\n",
    "        # Hard positive: Closest with same label\n",
    "        positive_indices = np.where(labels == anchor_label)[0]\n",
    "        positive_indices = positive_indices[positive_indices != i]  # Exclude self\n",
    "        if len(positive_indices) == 0:\n",
    "            continue\n",
    "        positive_idx = positive_indices[torch.argmin(pairwise_distances[i, positive_indices])]\n",
    "\n",
    "        # Hard negative: Furthest with different label\n",
    "        negative_indices = np.where(labels != anchor_label)[0]\n",
    "        negative_idx = negative_indices[torch.argmax(pairwise_distances[i, negative_indices])]\n",
    "\n",
    "        hard_triplets.append((anchor_idx, positive_idx, negative_idx))\n",
    "\n",
    "    return hard_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, dataloader, device):\n",
    "    '''\n",
    "    Extracts embeddings for images in the dataset.\n",
    "    '''\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    return torch.cat(all_embeddings), np.array(all_labels)\n",
    "\n",
    "def retrieve_top_k(embedding, dataset_embeddings, dataset_labels, k=5):\n",
    "    '''\n",
    "    Retrieve top-k similar images using Euclidean distance.\n",
    "    '''\n",
    "    distances = torch.cdist(embedding.unsqueeze(0), dataset_embeddings)\n",
    "    top_k_indices = torch.argsort(distances, dim=1)[0][:k]\n",
    "    return dataset_labels[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaizing model\n",
    "model = ViTEncoder(\n",
    "    embed_dim=256,\n",
    "    depth=4,\n",
    "    n_heads=8,\n",
    "    out_dim=512,\n",
    ")\n",
    "\n",
    "# defining transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_fn, device, epochs=10):\n",
    "    model.train()\n",
    "    train_loss_hist = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Extract embeddings\n",
    "            embeddings = model(images)\n",
    "\n",
    "            # Hard triplet selection\n",
    "            triplets = get_hard_triplets(embeddings, labels)\n",
    "            if len(triplets) == 0:\n",
    "                continue\n",
    "\n",
    "            anchor, positive, negative = zip(*triplets)\n",
    "            anchor = torch.stack([embeddings[i] for i in anchor])\n",
    "            positive = torch.stack([embeddings[i] for i in positive])\n",
    "            negative = torch.stack([embeddings[i] for i in negative])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(anchor, positive, negative)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= len(dataloader)\n",
    "        train_loss_hist.append(epoch_loss)\n",
    "        print(f'Epoch {epoch+1:3}/{epochs:3} | Loss: {epoch_loss:.4f}')\n",
    "    return train_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = VehicleReIDDataset(root_dir='data/prepared_VeRi_CARLA_dataset/test', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "loss_fn = TripletLoss(margin=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "train_loss_hist = train(model, dataloader, optimizer, loss_fn, DEVICE, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "torch.save(model.state_dict(), 'trained_models/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('trained_models/model.pth', map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, image_path, device=DEVICE, transform=transform):\n",
    "    '''\n",
    "    Extracts embeddings for a single image.\n",
    "    '''\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image)\n",
    "    return embedding.cpu().numpy().flatten()\n",
    "\n",
    "trial_image_path = './data/VeRi_CARLA_dataset/image_gallery/20220710050038_0_44.jpg'\n",
    "infer(model, trial_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('trained_models/model.pth', map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset = VehicleReIDDataset(root_dir='data/VeRi_CARLA_dataset/image_query', name='VeRi_CARLA', transform=transform)\n",
    "gallery_dataset = VehicleReIDDataset(root_dir='data/VeRi_CARLA_dataset/image_gallery', name='VeRi_CARLA', transform=transform)\n",
    "\n",
    "query_dataloader = DataLoader(query_dataset, batch_size=1, shuffle=False)\n",
    "gallery_dataloader = DataLoader(gallery_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "query_embeddings, query_labels = extract_embeddings(model, query_dataloader, DEVICE)\n",
    "gallery_embeddings, gallery_labels = extract_embeddings(model, gallery_dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "similarity = cosine_similarity(query_embeddings, gallery_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cmc(similarity_matrix, query_ids, gallery_ids, max_rank=10):\n",
    "    \"\"\"Computes CMC Curve & Rank-N Accuracy\"\"\"\n",
    "    num_queries = len(query_ids)\n",
    "    cmc_curve = np.zeros(max_rank)\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        # Sort gallery images by similarity score\n",
    "        sorted_indices = np.argsort(similarity_matrix[i])[::-1]\n",
    "\n",
    "        # Get ranked list of predicted IDs\n",
    "        ranked_vehicle_ids = gallery_ids[sorted_indices]\n",
    "\n",
    "        # Find rank of the first correct match\n",
    "        correct_match_ranks = np.where(ranked_vehicle_ids == query_ids[i])[0]\n",
    "\n",
    "        if len(correct_match_ranks) > 0:\n",
    "            first_correct_rank = correct_match_ranks[0]\n",
    "            cmc_curve[first_correct_rank:] += 1  # Increment all ranks ≥ first match\n",
    "\n",
    "    cmc_curve /= num_queries  # Normalize\n",
    "\n",
    "    # Rank-N Metrics\n",
    "    rank_1 = cmc_curve[0] * 100\n",
    "    rank_5 = cmc_curve[4] * 100 if max_rank >= 5 else None\n",
    "    rank_10 = cmc_curve[9] * 100 if max_rank >= 10 else None\n",
    "\n",
    "    return cmc_curve, rank_1, rank_5, rank_10\n",
    "\n",
    "cmc_curve, rank_1, rank_5, rank_10 = compute_cmc(similarity, query_labels, gallery_labels)\n",
    "\n",
    "print(f\"Rank-1 Accuracy: {rank_1:.2f}%\")\n",
    "print(f\"Rank-5 Accuracy: {rank_5:.2f}%\" if rank_5 else \"\")\n",
    "print(f\"Rank-10 Accuracy: {rank_10:.2f}%\" if rank_10 else \"\")\n",
    "\n",
    "ranks = np.arange(1, len(cmc_curve) + 1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(ranks, cmc_curve, marker=\"o\", linestyle=\"-\", label=\"CMC Curve\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Matching Accuracy\")\n",
    "plt.title(\"CMC Curve for Vehicle Re-Identification\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_map(similarity_matrix, query_ids, gallery_ids):\n",
    "    \"\"\"Computes mean Average Precision (mAP)\"\"\"\n",
    "    num_queries = len(query_ids)\n",
    "    average_precisions = []\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        # Sort gallery images by similarity score\n",
    "        sorted_indices = np.argsort(similarity_matrix[i])[::-1]\n",
    "\n",
    "        # Get binary relevance vector (1 if correct, 0 otherwise)\n",
    "        relevance = (gallery_ids[sorted_indices] == query_ids[i]).astype(int)\n",
    "\n",
    "        # Compute Average Precision (AP)\n",
    "        if relevance.sum() > 0:\n",
    "            ap = average_precision_score(relevance, similarity_matrix[i, sorted_indices])\n",
    "            average_precisions.append(ap)\n",
    "\n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "map_score = compute_map(similarity, query_labels, gallery_labels)\n",
    "print(f'Mean Average Precision (mAP): {map_score * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
